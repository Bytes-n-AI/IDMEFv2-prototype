filter {
  if "alert" not in [tags] {
    drop {}
  }

  uuid {
    target => "ID"
  }

  mutate {
    replace => {
      '[Version]' => '2.0.3'
    }
  }

  if [@metadata][kafka][key] != "IDMEFv2" {
    ruby {
      path => "/usr/share/logstash/config/scripts/fill_alert.rb"
    }

    ruby {
      code => "
        event.set('[CreateTime]', event.get('@timestamp').to_s)

        # Convert Confidence to a number
        v = event.get('[Confidence]')
        event.set('[Confidence]', v.to_f) unless v.nil?

        # Convert hashes back into lists.
        [
          '[Category]', '[AltNames]', '[AltCategory]', '[Ref]', '[CorrelID]', '[AggrCondition]', '[PredID]', '[RelID]',
          '[Analyzer][Category]', '[Analyzer][Data]', '[Analyzer][Method]',
          '[Sensor]', '[Source]', '[Target]', '[Vector]',
        ].each do |k|
          v = event.get(k)
          if v.is_a?(Hash) then
            v = v.values
            event.set(k, v)
          end

          # If dealing with a list of hashes, look for sub-hashes and convert them to lists too.
          if v.is_a?(Array) then
            v.each_with_index do |e, idx|
              if e.is_a?(Hash)
                e.each do |sk, sv|
                  if sv.is_a?(Hash) then
                    sv = sv.values

                    # Convert Source(*).Port(*)/Target(*).Port(*) to a list of integers.
                    if ['[Source]', '[Target]'].include?(k) and sk == 'Port'
                      begin
                        sv.map! { |n| n.to_i }
                      rescue => e
                      end
                    end

                    event.set(k+'['+idx.to_s+']['+sk+']', sv)
                  end
                end
              end
            end
          end
        end

        unless event.get('[Analyzer][Category]')
          event.set('[Analyzer][Category]', ['LOG', 'ETL'])
        end

        unless event.get('[Analyzer][Data]')
          event.set('[Analyzer][Data]', ['Log'])
        end

        unless event.get('[Analyzer][Method]')
          event.set('[Analyzer][Method]', ['Signature', 'Monitor'])
        end

        # Merge the TI into the Source(*).
        ['IP', 'Email'].each do |ti_type|
          ti_data = (event.get('[Attachment][CTI][Content]') || {}).fetch(ti_type, {})
          (event.get('[Source]') || []).each_with_index do |source, idx|
            v = source[ti_type]
            unless v.nil?
              ti = ti_data[v]
              if ti
                idmefv2_ti = source['TI'] || []
                attachments = source['Attachment'] || []
                event.set('[Source]['+idx.to_s+'][TI]', idmefv2_ti | ti)
                event.set('[Source]['+idx.to_s+'][Attachment]', attachments | ['CTI'])
              end
            end
          end
        end
      "
    }
  }

  prune {
    whitelist_names => [
      # Top-level IDMEF attributes
      'Version',
      'ID',
      'Entity',
      'Category',
      'Cause',
      'Description',
      'Status',
      'Severity',
      'Confidence',
      'Note',
      'CreateTime',
      'StartTime',
      'CeaseTime',
      'DeleteTime',
      'AltNames',
      'AltCategory',
      'Ref',
      'CorrelID',
      'AggrCondition',
      'PredID',
      'RelID',

      # IDMEF classes
      'Analyzer',
      'Sensor',
      'Source',
      'Target',
      'Vector',
      'Attachment',
      'Observable'
    ]

    add_field       => {
      "[@metadata][validate]" => "${VALIDATE:yes}"
    }
  }

  if [@metadata][validate] != "no" {
    schema_check {
      schema_path      => "/usr/share/logstash/config/IDMEFv2-light.schema"
      refresh_interval => 300
      debug_output     => true
      tag_on_failure   => "_schemacheckfailure"
      add_field        => {
        "[@metadata][valid]" => "yes"
      }
    }
  }
}

output {
  if "_schemacheckfailure" in [tags] {
    stdout { codec => json }
  } else {
    kafka {
      bootstrap_servers => "${BOOTSTRAP_SERVER}"
      client_id         => "output-detected"
      topic_id          => "${TOPIC_DETECTED}"
      codec             => json
      partitioner       => "uniform_sticky"
      message_key       => "%{[@metadata][kafka][key]}"
    }
  }
}
