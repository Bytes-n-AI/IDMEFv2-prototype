input {
  kafka {
    bootstrap_servers => "${BOOTSTRAP_SERVER}"
    client_id         => "collected_analyzed"
    group_id         => "collected_analyzed"
    topics            => ["${TOPIC_COLLECTED}_cleaned"]
    codec             => json
  }
}

filter {
  if [Attachment][RawLog][Content][process][name] == "CEF" {
    # Jan 18 11:07:53 host CEF: 0|Citrix|NetScaler|NS10.0|APPFW|APPFW_STARTURL|6|src=10.217.253.78 spt=54711 requestMethod=GET request=http://vpx247.example.net/FFC/login_post.html?abc\=def msg=Disallow Illegal URL. cn1=465 cn2=535 cs1=profile1 cs2=PPE0 cs3=IliG4Dxp1SjOhKVRDVBXmqvAaIcA000 cs4=ALERT cs5=2012 act=not blocked
    ruby {
      init => "
        require 'logstash/codecs/cef'
        class CEF < LogStash::Codecs::CEF
          attr_reader :decode_mapping
          attr_reader :header_fields
        end
        @cef = CEF.new({'ecs_compatibility' => 'v1'})
      "
      code => "
        @cef.handle(event.get('[Attachment][RawLog][Content][message]')) do |ev|
          tags = ev.get('tags')
          if tags.nil? or !tags.to_a.contains('_cefparsefailure')
            # See generate_mappings! in https://github.com/logstash-plugins/logstash-codec-cef/blob/master/lib/logstash/codecs/cef.rb
            @cef.decode_mapping.each_key do |cefName|
              v = ev.get(cefName)
              unless v.nil?
                event.set('[Attachment][RawLog][Content][CEF]['+cefName+']', v)
              end
            end
            @cef.header_fields.each do |cefName|
              v = ev.get(cefName)
              unless v.nil?
                event.set('[Attachment][RawLog][Content][CEF]['+cefName+']', v)
              end
            end
          end
          break
        end
      "
    }
  }

  # Extracts things that look like IP addresses, Email addresses and URIs
  # so CTI enrichment can analyze them.
  ruby {
    path          => "/usr/share/logstash/config/scripts/extraction.rb"
    script_params => {
      'source' => '[Attachment][RawLog][Content][message]'
    }
  }

  if [Attachment][RawLog][Content][host][hostname] {
    mutate {
      add_field => {
        '[Sensor][0][Hostname]' => '%{[Attachment][RawLog][Content][host][hostname]}'
      }
    }
  }

  mutate {
    add_field => {
      '[Sensor][0][IP]'       => '%{[Attachment][RawLog][Content][host][ip]}'
      '[Sensor][0][Model]'    => '%{[Attachment][RawLog][Content][service][type]}'
      '[Sensor][0][Name]'     => '%{[Attachment][RawLog][Content][service][type]}'
    }
  }

  prune {
    whitelist_names => [
      # Top-level IDMEF attributes
      'Sensor',
      'Attachment'
    ]
  }
}

output {
  kafka {
    bootstrap_servers => "${BOOTSTRAP_SERVER}"
    topic_id => "${TOPIC_COLLECTED}_analyzed"
    codec => json
  }
}
