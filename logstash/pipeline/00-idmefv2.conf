input {
  http {
    port                         => 6515
    max_content_length           => "${MAX_MSG_SIZE}"
    response_code                => 204
    ecs_compatibility            => disabled
    request_headers_target_field => "[@metadata][headers]"
    remote_host_target_field     => "[@metadata][ip_address]"
    additional_codecs            => {
      "application/json" => "json"
      "text/json"        => "json"
    }
    add_field                    => {
      "[@metadata][source]" => "IDMEFv2"
    }
  }

  kafka {
    bootstrap_servers => "${BOOTSTRAP_SERVER}"
    client_id         => "input-internal"
    group_id          => "input-internal"
    topics            => [
      "prelude-correlator",
      "prelude-ai"
    ]
    codec             => json
    add_field         => {
      "[@metadata][source]"                => "IDMEFv2"
      "[@metadata][headers][content_type]" => "application/json"
    }
  }
}

filter {
  prune {
    blacklist_names => ['^@']
  }

  if [@metadata][headers][content_type] not in ['text/json', 'application/json'] {
    drop { }
  }

  schema_check {
    schema_path      => "/usr/share/logstash/config/IDMEFv2.schema"
    refresh_interval => 300
    debug_output     => true
    tag_on_failure   => "_schemacheckfailure"
  }

  if "_schemacheckfailure" not in [tags] {
    ruby {
      # Convert Attachments & Observables into hashes to make them easier to process.
      code => "
      ['Attachment', 'Observable'].each do |field|
        entries = {}
        (event.get(field) || []).each do |v|
          k = v.delete('Name')
          entries[k] = v
        end
        unless entries.empty?
          event.set(field, entries)
        end
      end
      "
    }
  }
}

output {
  if "_schemacheckfailure" in [tags] {
    stdout { codec => json }
  } else {
    pipeline { send_to => "output-norm" }
  }
}
